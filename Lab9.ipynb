{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificarea CIFAR-10 cu Augmentarea Datelor\n",
    "\n",
    "În acest exercițiu, ne întoarcem la CIFAR-10 și la rețelele pe care le-am construit anterior. Vom folosi augmentarea datelor în timp real pentru a încerca să îmbunătățim rezultatele noastre.\n",
    "\n",
    "Odată ce ați terminat de parcurs caietul, experimentați cu diferite parametri de augmentare a datelor și vedeți dacă aceștia ajută (sau dimpotrivă!) performanța clasificatorului dumneavoastră."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Datele, amestecate și împărțite între seturile de antrenare și testare:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În Exercițiul 6, am construit două modele. Unul era mai mic (cu 181K parametri), în timp ce al doilea era mai mare (cu 1.25M parametri). Mai jos folosim modelul mai mic și îl antrenăm cu augmentare de date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 16, 32)        2432      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 6, 6, 32)          25632     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 3, 3, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 3, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 288)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               147968    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,162\n",
      "Trainable params: 181,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Să construim un CNN folosind capabilitățile secvențiale ale lui Keras\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "\n",
    "## Convoluție 5x5 cu pas de 2x2 și 32 de filtre\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## O altă convoluție 5x5 cu pas de 2x2 și 32 de filtre\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## Max pooling 2x2 reduce la 3 x 3 x 32\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "## Flatten transformă 3x3x32 în 288x1\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(512))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avem încă 181K parametri, chiar dacă acesta este un model \"mic\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# inițializăm optimizatorul RMSprop\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.0005, decay=1e-6)\n",
    "\n",
    "# Hai să antrenăm modelul folosind RMSprop\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aici definim `ImageDataGenerator` pe care îl vom folosi pentru a furniza imagini modelului nostru în timpul procesului de antrenare. În prezent, este configurat să facă câteva deplasări și să inverseze orizontal imaginile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 1.6752 - accuracy: 0.3881 - val_loss: 2.1454 - val_accuracy: 0.2472\n",
      "Epoch 2/15\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 1.4498 - accuracy: 0.4754 - val_loss: 2.3423 - val_accuracy: 0.2341\n",
      "Epoch 3/15\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 1.3414 - accuracy: 0.5197 - val_loss: 2.0809 - val_accuracy: 0.3061\n",
      "Epoch 4/15\n",
      "1562/1562 [==============================] - 45s 29ms/step - loss: 1.2815 - accuracy: 0.5473 - val_loss: 2.0467 - val_accuracy: 0.3349\n",
      "Epoch 5/15\n",
      "1562/1562 [==============================] - 43s 28ms/step - loss: 1.2412 - accuracy: 0.5618 - val_loss: 1.7553 - val_accuracy: 0.3719\n",
      "Epoch 6/15\n",
      "1562/1562 [==============================] - 45s 29ms/step - loss: 1.2214 - accuracy: 0.5694 - val_loss: 2.0131 - val_accuracy: 0.3360\n",
      "Epoch 7/15\n",
      "1562/1562 [==============================] - 51s 33ms/step - loss: 1.1967 - accuracy: 0.5796 - val_loss: 1.8426 - val_accuracy: 0.3519\n",
      "Epoch 8/15\n",
      "1562/1562 [==============================] - 43s 28ms/step - loss: 1.1815 - accuracy: 0.5897 - val_loss: 1.7269 - val_accuracy: 0.3921\n",
      "Epoch 9/15\n",
      "1562/1562 [==============================] - 43s 28ms/step - loss: 1.1757 - accuracy: 0.5910 - val_loss: 1.6478 - val_accuracy: 0.4064\n",
      "Epoch 10/15\n",
      "1562/1562 [==============================] - 43s 27ms/step - loss: 1.1748 - accuracy: 0.5927 - val_loss: 1.9773 - val_accuracy: 0.3878\n",
      "Epoch 11/15\n",
      "1562/1562 [==============================] - 47s 30ms/step - loss: 1.1695 - accuracy: 0.5990 - val_loss: 2.2788 - val_accuracy: 0.3227\n",
      "Epoch 12/15\n",
      "1562/1562 [==============================] - 44s 28ms/step - loss: 1.1668 - accuracy: 0.5990 - val_loss: 1.9246 - val_accuracy: 0.4052\n",
      "Epoch 13/15\n",
      "1562/1562 [==============================] - 44s 28ms/step - loss: 1.1591 - accuracy: 0.6024 - val_loss: 1.9718 - val_accuracy: 0.4279\n",
      "Epoch 14/15\n",
      "1562/1562 [==============================] - 45s 29ms/step - loss: 1.1676 - accuracy: 0.5997 - val_loss: 1.7686 - val_accuracy: 0.3759\n",
      "Epoch 15/15\n",
      "1562/1562 [==============================] - 51s 32ms/step - loss: 1.1631 - accuracy: 0.6046 - val_loss: 1.5717 - val_accuracy: 0.4513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ef02053ad0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # setăm media intrării la 0 peste setul de date\n",
    "    samplewise_center=True,    # setează media fiecărui eșantion la 0\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False)\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model_1.fit(datagen.flow(x_train, y_train,\n",
    "                                    batch_size=batch_size),\n",
    "                      steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                      epochs=15,\n",
    "                      validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cum se compară performanța cu antrenarea fără augmentare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Performanța modelului antrenat cu augmentare de date poate varia în funcție de setul de date și de arhitectura modelului.\n",
    "În general, augmentarea datelor poate ajuta la îmbunătățirea generalizării și la reducerea suprapunerii în antrenare, ceea ce poate duce la rezultate mai bune pe setul de testare.\n",
    "Cu toate acestea, efectul exact poate fi diferit în funcție de scenariu.\n",
    "Pentru a evalua exact diferența de performanță, este necesar să comparați metricile de evaluare relevante (cum ar fi acuratețea sau pierderea) între modelul antrenat cu augmentare de date și modelul antrenat fără augmentare, folosind același set de date de testare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Exercițiu\n",
    "### Rândul tău\n",
    "\n",
    "1. Experimentează mai sus cu diferite setări ale parametrilor de augmentare a datelor. Poți să faci modelul să se comporte mai bine? Poți să-l faci să se comporte mai rău?\n",
    "\n",
    "2. Așa cum ai făcut în Exercițiul 6, Construiește un model mai complicat cu următorul pattern:\n",
    "   - Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Clasificare Finală\n",
    "   - Folosește pași de 1 pentru toate straturile de convoluție.\n",
    "\n",
    "3. Folosește augmentarea datelor pentru a antrena acest model. Poți obține o performanță mai bună?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hai să construim o rețea neurală convoluțională folosind capabilitățile secvențiale ale Keras\n",
    "\n",
    "# Scrie codul tău aici\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 10, 10, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               819712    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 890,410\n",
      "Trainable params: 890,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Verifica numarul de parametri (afiseaza summary-ul)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inițializăm optimizatorul RMSprop\n",
    "\n",
    "# Hai să antrenăm modelul folosind RMSprop\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1562/1562 [==============================] - 200s 127ms/step - loss: 1.4685 - accuracy: 0.4704 - val_loss: 1.7443 - val_accuracy: 0.3846\n",
      "Epoch 2/15\n",
      "1562/1562 [==============================] - 191s 122ms/step - loss: 1.0748 - accuracy: 0.6227 - val_loss: 1.1999 - val_accuracy: 0.5726\n",
      "Epoch 3/15\n",
      "1562/1562 [==============================] - 192s 123ms/step - loss: 0.9320 - accuracy: 0.6773 - val_loss: 1.0415 - val_accuracy: 0.6428\n",
      "Epoch 4/15\n",
      "1562/1562 [==============================] - 193s 124ms/step - loss: 0.8828 - accuracy: 0.6988 - val_loss: 1.3499 - val_accuracy: 0.5557\n",
      "Epoch 5/15\n",
      "1562/1562 [==============================] - 191s 122ms/step - loss: 0.8621 - accuracy: 0.7095 - val_loss: 2.5690 - val_accuracy: 0.3966\n",
      "Epoch 6/15\n",
      "1562/1562 [==============================] - 191s 122ms/step - loss: 0.8620 - accuracy: 0.7094 - val_loss: 2.9927 - val_accuracy: 0.4710\n",
      "Epoch 7/15\n",
      "1562/1562 [==============================] - 188s 120ms/step - loss: 0.8692 - accuracy: 0.7087 - val_loss: 1.4022 - val_accuracy: 0.5718\n",
      "Epoch 8/15\n",
      "1562/1562 [==============================] - 193s 123ms/step - loss: 0.8712 - accuracy: 0.7122 - val_loss: 1.2026 - val_accuracy: 0.6164\n",
      "Epoch 9/15\n",
      "1562/1562 [==============================] - 196s 125ms/step - loss: 0.8737 - accuracy: 0.7122 - val_loss: 2.2351 - val_accuracy: 0.5576\n",
      "Epoch 10/15\n",
      "1562/1562 [==============================] - 192s 123ms/step - loss: 0.8877 - accuracy: 0.7065 - val_loss: 1.4299 - val_accuracy: 0.5052\n",
      "Epoch 11/15\n",
      "1562/1562 [==============================] - 182s 116ms/step - loss: 0.8833 - accuracy: 0.7079 - val_loss: 1.3074 - val_accuracy: 0.5993\n",
      "Epoch 12/15\n",
      "1562/1562 [==============================] - 193s 124ms/step - loss: 0.9031 - accuracy: 0.7037 - val_loss: 1.3271 - val_accuracy: 0.5644\n",
      "Epoch 13/15\n",
      "1562/1562 [==============================] - 184s 118ms/step - loss: 0.9025 - accuracy: 0.7038 - val_loss: 1.4027 - val_accuracy: 0.5586\n",
      "Epoch 14/15\n",
      "1562/1562 [==============================] - 174s 111ms/step - loss: 0.9075 - accuracy: 0.7028 - val_loss: 1.0356 - val_accuracy: 0.6570\n",
      "Epoch 15/15\n",
      "1562/1562 [==============================] - 235s 150ms/step - loss: 0.9219 - accuracy: 0.7012 - val_loss: 1.3016 - val_accuracy: 0.5784\n"
     ]
    }
   ],
   "source": [
    "# Calculăm cantitățile necesare pentru normalizarea pe bază de caracteristici\n",
    "\n",
    "# Antrenăm modelul pe loturile generate de datagen.flow().\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=True,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
    "                              steps_per_epoch=x_train.shape[0] // 32,\n",
    "                              epochs=15,\n",
    "                              validation_data=(x_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
